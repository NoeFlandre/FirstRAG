


# Import Langchain modules
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field

# Other modules and packages
import os
import tempfile
import pandas as pd
from dotenv import load_dotenv





load_dotenv()


OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")





llm = ChatOpenAI(model="gpt-4o-mini", api_key=OPENAI_API_KEY)








loader = PyPDFLoader("data/wsc23paper.pdf")
pages = loader.load()





text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200, length_function=len, separators= ["\n\n", "\n", " "])
chunks = text_splitter.split_documents(pages)





def get_embedding_function():
    embeddings = OpenAIEmbeddings(
        model = "text-embedding-3-large", openai_api_key=OPENAI_API_KEY
    )
    return embeddings
embedding_function = get_embedding_function()





import uuid

def create_vectorstore(chunks, embedding_function, vectorstore_path):

    # Create a list of unique ids for each document based on the content
    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]
    
    # Ensure that only unique docs with unique ids are kept
    unique_ids = set()
    unique_chunks = []
    
    for chunk, id in zip(chunks, ids):     
        if id not in unique_ids:       
            unique_ids.add(id)
            unique_chunks.append(chunk) 

    # Create a new Chroma database from the documents
    vectorstore = Chroma.from_documents(documents=unique_chunks, 
                                        ids=list(unique_ids),
                                        embedding=embedding_function, 
                                        persist_directory = vectorstore_path)

    vectorstore.persist()
    
    return vectorstore


# Create vectorstore
vectorstore = create_vectorstore(chunks=chunks, 
                                 embedding_function=embedding_function, 
                                 vectorstore_path="vectorstore3")








vectorstore = Chroma(persist_directory="vectorstore3", embedding_function=embedding_function)





retriever = vectorstore.as_retriever(search_type="similarity")






PROMPT = """
You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to
answer the question. If you don't know the answer, 
just say that you don't know, don't try to make up 
an answer.

Here is the context : {context}


Here is the question : {question}

"""








question = "Who is the author of this article?"





relevant_chunks = retriever.invoke(question)
context_text = "\n\n---\n\n".join([chunk.page_content for chunk in relevant_chunks])





prompt_template = ChatPromptTemplate.from_template(PROMPT)
prompt = prompt_template.format(
    question=question, context=context_text)

print(prompt)





llm.invoke(prompt)





def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)



rag_chain = (
            {"context": retriever | format_docs, "question": RunnablePassthrough()}
            | prompt_template
            | llm
        )
rag_chain.invoke("Who is the author of this article?")






